{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pong AI.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/rubencg195/PongAI/blob/master/Pong_AI.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "wD1rxGKUCPes",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Source:\n",
        "\n",
        "https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0\n",
        "\n",
        "\n",
        "#Problem\n",
        "\n",
        "We are given the following:\n",
        "\n",
        "1. A sequence of images (frames) representing each frame of the Pong game.\n",
        "2. An indication when we’ve won or lost the game.\n",
        "3. An opponent agent that is the traditional Pong computer player.\n",
        "4. An agent we control that we can tell to move up or down at each frame.\n",
        "\n",
        "#Solution\n",
        "\n",
        "![The architecture Of Andrej’s solution from his blog post.](pong_assets/1.png)\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*05ExQKJ0nOoWV80SNVEyJg.png)\n",
        "\n",
        "\n",
        "Our Neural Network, based heavily on Andrej’s solution, will do the following:\n",
        "\n",
        "1. Take in images from the game and preprocess them (remove color, background, downsample etc.).\n",
        "2. Use the Neural Network to compute a probability of moving up.\n",
        "3. Sample from that probability distribution and tell the agent to move up or down.\n",
        "4. If the round is over (you missed the ball or the opponent missed the ball), find whether you won or lost.\n",
        "5. When the episode has finished(someone got to 21 points), pass the result through the backpropagation algorithm to compute the gradient for our weights.\n",
        "6. After 10 episodes have finished, sum up the gradient and move the weights in the direction of the gradient.\n",
        "7. Repeat this process until our weights are tuned to the point where we can beat the computer. That’s basically it! Let’s start looking at how our code achieves this."
      ]
    },
    {
      "metadata": {
        "id": "XYDns8BdCPew",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import gym\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NZGwLrs8CPe1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Make the environment and get the first image of the game\n",
        "env         = gym.make(\"Pong-v0\")\n",
        "observation = env.reset()\n",
        "\n",
        "#HyperParameters\n",
        "\n",
        "#How many episodes to wait before moving the weights\n",
        "batch_size               = 10  \n",
        "#Discount factor to discount the effect of old actions on the final result\n",
        "gamma                    = 0.99\n",
        "#Parameter used in RMSProp algorithm\n",
        "decay_rate               = 0.99\n",
        "#Hoy many neurons ar in hidden layer\n",
        "num_hidden_layer_neurons = 200\n",
        "#Dimensions of our observation image\n",
        "input_dimensions         = 80*80\n",
        "#The rate at which we learn from our results to compute the new weights\n",
        "#A higher rate means we react more to results and a lower rate means \n",
        "#we don't react as strongly to each result\n",
        "learning_rate            = 1e-4\n",
        "\n",
        "episode_number             = 0\n",
        "reward_sum                 = 0\n",
        "running_reward             = None\n",
        "prev_processed_observation = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "umin3D-CR2Lw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Initialize Weights"
      ]
    },
    {
      "metadata": {
        "id": "uozA9MDkCPe4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Layers\n",
        "\n",
        "#               Dimension            Result\n",
        "#Observations  6400x1    Matrix\n",
        "#Layer 1       200x6400  Matrix      200x1\n",
        "#Layer 2       1x200     Matrix      1x1\n",
        "\n",
        "#Initialize Weights\n",
        "weights = {\n",
        "    '1': np.random.randn(num_hidden_layer_neurons, input_dimensions) / np.sqrt(input_dimensions)         ,\n",
        "    '2': np.random.randn(num_hidden_layer_neurons)                   / np.sqrt(num_hidden_layer_neurons)\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u7VRczPBSCd9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Initialize RMS Props Parameters and Historic Values´ Containers"
      ]
    },
    {
      "metadata": {
        "id": "yfLQWiloSKQT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Initial Parameters for RMSProp Algorithm\n",
        "#(http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop)\n",
        "expectation_g_squared     = {}\n",
        "g_dict                    = {}\n",
        "for layer_name  in weights.keys():\n",
        "    expectation_g_squared[layer_name] = np.zeros_like(weights[layer_name])\n",
        "    g_dict[layer_name]                = np.zeros_like(weights[layer_name])\n",
        "\n",
        "#Array to collect Observations and Intermidiate Values across to\n",
        "#compute the gradient at the end based on the result\n",
        "episode_hidden_layer_values,    \\\n",
        "episode_onservations,           \\\n",
        "episode_gradient_log_ps,        \\\n",
        "episode_rewards = [],[],[],[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KgWCLYLYCPe8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Pre-processed the Image Frames\n",
        "\n",
        "Let’s dive into preprocess_observations to see how we convert the image OpenAI Gym gives us into something we can use to train our Neural Network. The basic steps are:\n",
        "\n",
        "1. Crop the image (we just care about the parts with information we care about).\n",
        "2. Downsample the image. (Shrinking)\n",
        "3. Convert the image to black and white (color is not particularly important to us).\n",
        "4. Remove the background.\n",
        "5. Convert from an 80 x 80 matrix of values to 6400 x 1 matrix (flatten the matrix so it’s easier to use).\n",
        "6. Store just the difference between the current frame and the previous frame if we know the previous frame (we only care about what’s changed)."
      ]
    },
    {
      "metadata": {
        "id": "UXSYVn4ACPe9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def downsample(image):\n",
        "  #Take only alternate pixels \n",
        "  #Basicaly halves the resolution of the image (which is fine for us)\n",
        "  return image[::2, ::2, :]\n",
        "\n",
        "def remove_color(image):\n",
        "  #Convert all color (RGB is the third dimension in the image)\n",
        "  return image[:, :, 0]\n",
        "\n",
        "def remove_background(image):\n",
        "  image[image == 144] = 0\n",
        "  image[image == 109] = 0\n",
        "  return image\n",
        "\n",
        "def preprocess_observation(input_observation, prev_observation, input_dimension):\n",
        "    \"\"\"Convert the 210x160x3 uint8 frame into a 6400 flout vector\"\"\"\n",
        "    processed_observation = input_observation[35:195]                  #Crop\n",
        "    processed_observation = downsapme(processed_observation)\n",
        "    processed_observation = remove_color(processed_observation)\n",
        "    processed_observation = remove_background(processed_observation)\n",
        "    \n",
        "    #Set everything left to 1 (paddles, ball)\n",
        "    processed_observation[processed_observation != 0] = 1\n",
        "    \n",
        "    #Substract the previous frame from the current one so we are only processing on changes in the game\n",
        "    if prev_processed_observation is not None:\n",
        "      input_observation = processed_observation - prev_processed_observation\n",
        "    else:\n",
        "      input_observation = np.zeros(input_dimensions)\n",
        "    \n",
        "    #Store the previous frame so we can substract from it next time\n",
        "    prev_processed_observation = processed_observation\n",
        "    \n",
        "    return input_observation, prev_processed_observation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J0xPfIrsJdVZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Forward Propagation\n",
        "\n",
        "\n",
        "1. Compute the unprocessed hidden layer values by simply finding the dot product of the weights[1] (weights of layer 1) and the observation_matrix. We have 200 neurons so each row represents the output of one neuron.\n",
        "\n",
        "  **HiddenLayer(200x1) = W1(200x6400) * ObservationMatrix(6400x1) **\n",
        "\n",
        "2. Next, we apply a non linear thresholding function on those hidden layer values - in this case just a simple ReLU. At a high level, this introduces the nonlinearities that makes our network capable of computing nonlinear functions rather than just simple linear ones.\n",
        "\n",
        "3. We use those hidden layer activation values to calculate the output layer values. \n",
        "\n",
        "  **OutputLayer(1x1) = W1(1x200) * HiddenLayer(200x1) **\n",
        "\n",
        "4. Finally, we apply a sigmoid function on this output value so that it’s between 0 and 1 and is therefore a valid probability (probability of going up)."
      ]
    },
    {
      "metadata": {
        "id": "RJsTIfd-CPe4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Matrix Multiplication.](pong_assets/2.png)\n",
        "![Matrix Multiplication.](pong_assets/3.png)"
      ]
    },
    {
      "metadata": {
        "id": "_rRy6GOrG93Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def relu(vector):\n",
        "  vector[vector < 0] = 0\n",
        "  return vector\n",
        "\n",
        "def apply_neural_nets(observation_matrix, weights):\n",
        "  ##Based on the obsevation_matrix and weights, compute the new hidden layer values and the new output layer values\n",
        "  hidden_layer_values = np.dot(weights['1'], observation_matrix)\n",
        "  hidden_layer_values = relu(hidden_layer_values)\n",
        "  hidden_layer_values = np.dot(hidden_layer_values)\n",
        "  \n",
        "  output_layer_values = np.dot(hidden_layer_values, weights['2'])\n",
        "  output_layer_values = sigmoid(output_layer_values)\n",
        "  return hidden_layer_values, output_layer_values\n",
        "\n",
        "def choose_action(probability):\n",
        "    random_value = np.random.uniform()\n",
        "    if random_value < probability:\n",
        "        # signifies up in openai gym\n",
        "        return 2\n",
        "    else:\n",
        "         # signifies down in openai gym\n",
        "        return 3\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4LEhgU7jEERG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Discount the Rewards\n",
        "\n",
        "If you moved up at the first frame of the episode, it probably had very little impact on whether or not you win. However, **closer to the end of the episode, your actions probably have a much larger effect as they determine whether or not your paddle reaches the ball and how your paddle hits the ball.**"
      ]
    },
    {
      "metadata": {
        "id": "JGHcTDFuENG2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def discount_rewards(rewards, gamma):\n",
        "    \"\"\" Actions you took 20 steps before the end result are less important to the overall result than an action you took a step ago.\n",
        "    This implements that logic by discounting the reward on previous actions based on how long ago they were taken\"\"\"\n",
        "    discounted_rewards = np.zeros_like(rewards)\n",
        "    running_add = 0\n",
        "    for t in reversed(xrange(0, rewards.size)):\n",
        "        if rewards[t] != 0:\n",
        "            running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "        running_add = running_add * gamma + rewards[t]\n",
        "        discounted_rewards[t] = running_add\n",
        "    return discounted_rewards\n",
        "\n",
        "def discount_with_rewards(gradient_log_p, episode_rewards, gamma):\n",
        "    \"\"\" discount the gradient with the normalized rewards \"\"\"\n",
        "    discounted_episode_rewards = discount_rewards(episode_rewards, gamma)\n",
        "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "    discounted_episode_rewards -= np.mean(discounted_episode_rewards)\n",
        "    discounted_episode_rewards /= np.std(discounted_episode_rewards)\n",
        "    return gradient_log_p * discounted_episode_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KZTVrCL7SHCI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Learning\n",
        "\n",
        "\n",
        "Learning is all about seeing the result of the action (i.e. whether or not we won the round) and changing our weights accordingly. The first step to learning is asking the following question:\n",
        "\n",
        "\n",
        "**How does changing the output probability (of going up) affect my result of winning the round?**\n",
        "Mathematically, this is just the derivative of our result with respect to the outputs of our final layer. If **L** is the value of our result to us and **f** is the function that gives us the activations of our final layer, this derivative is just **∂L/∂f.**\n",
        "\n",
        "\n",
        "In a binary classification context (i.e. we just have to tell the AI one of two actions, up or down), this derivative turns out to be\n",
        "\n",
        "![alt text](https://image.ibb.co/g5eEAT/image.png)\n",
        "\n",
        "Note that **σ** in the above equation represents the sigmoid function. Read the Attribute Classification section here:\n",
        "\n",
        "http://cs231n.github.io/neural-networks-2/#losses\n",
        "\n",
        "for more information about how we get the above derivative. We simplify this further below:\n",
        "\n",
        "**∂L/∂f = true_label(0 or 1) — predicted_label(0 or 1)**\n",
        "\n",
        "\n",
        "\n",
        "After one action(moving the paddle up or down), we don’t really have an idea of whether or not this was the right action. So we’re going to cheat and treat the action we end up sampling from our probability as the correct action.\n",
        "\n",
        "\n",
        "After calculating gradient per action, the next step is to figure out how we learn after the end of an episode (i.e. when we or our opponent miss the ball and someone gets a point). We do this by computing the policy gradient of the network at the end of each episode. The intuition here is that if we won the round, we’d like our network to generate more of the actions that led to us winning. Alternatively, if we lose, we’re going to try and generate less of these actions.\n",
        "\n",
        "OpenAI Gym provides us the handy done variable to tell us when an episode finishes (i.e. we missed the ball or our opponent missed the ball). When we notice we are done, the first thing we do is compile all our observations and gradient calculations for the episode. This allows us to apply our learnings over all the actions in the episode."
      ]
    },
    {
      "metadata": {
        "id": "AdwojrN0ZLrA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Use backpropagation to compute the gradient (i.e. the direction we need to move our weights to improve).**\n",
        "\n",
        "\n",
        "As you’ll see in that excerpt, there are four fundamental equations of backpropogation, a technique for computing the gradient for our weights.\n",
        "\n",
        "\n",
        "![alt text](https://image.ibb.co/fouRQT/image.png)\n",
        "\n",
        "\n",
        "Our goal is to find ∂C/∂w1 (BP4), the derivative of the cost function with respect to the first layer’s weights, and ∂C/∂w2, the derivative of the cost function with respect to the second layer’s weights. These gradients will help us understand what direction to move our weights in for the greatest improvement.\n",
        "\n",
        "To begin with, let’s start with **∂C/∂w2**. If **a^l2** is the activations of the hidden layer (layer 2), we see that the formula is:\n",
        "\n",
        "![alt text](https://image.ibb.co/cjWHX8/image.png)\n",
        "\n",
        "Next, we need to calculate ∂C/∂w1. The formula for that is:\n",
        "\n",
        "![alt text](https://image.ibb.co/bLSKeo/image.png)\n",
        "\n",
        "\n",
        "So all we need now is δ^l2. Once we have that, we can calculate ∂C/∂w1 and return.\n",
        "\n",
        "\n",
        "http://neuralnetworksanddeeplearning.com/chap2.html\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7t3Zm1FoZMC3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_gradient(gradient_log_p, hidden_layer_values, observation_values, weights):\n",
        "  \"\"\"See here: http://neuralnetworksanddeeplearning.com/chap2.html\"\"\"\n",
        "  delta_L = gradient_log_p\n",
        "  dC_dw2  = np.dot(hidden_layer_values.T, delta_L).ravel()\n",
        "  \n",
        "  delta_l2 = np.outer(delta_L, weights['2'])\n",
        "  delta_l2 = relu(delta_l2)\n",
        "  dC_dw1   = np.dot(delta_l2.T, observation_values)\n",
        "  \n",
        "  return {\n",
        "      '1': dC_dw1,\n",
        "      '2': dC_dw2\n",
        "  }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NTBIGihPdr7y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Update Weights\n",
        "\n",
        "To update the weights, we simply apply RMSProp, an algorithm for updating weights described by Sebastian Reuder here.\n",
        "\n",
        "![alt text](https://image.ibb.co/eQFuUo/image.png)\n",
        "\n",
        "http://ruder.io/optimizing-gradient-descent/index.html#rmsprop"
      ]
    },
    {
      "metadata": {
        "id": "i8SzATIqdrLT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def update_weights(weights, expectation_g_squared, g_dict, decay_rate, learning_rate):\n",
        "  \"\"\"See here: http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop\"\"\"\n",
        "  epsilon = 1e-5\n",
        "  \n",
        "  for layer_name in weights-keys():\n",
        "    g                                 =  g_dict[layer_name] \n",
        "    expectation_g_squared[layer_name] =  decay_rate * expectation_g_squared[layer_name] + ( 1 - decay_rate ) * g**2\n",
        "    weights[layer_name]               += (learning_rate * g)/(np.sqrt(expectation_g_squared[layer_name] + epsilon))\n",
        "    \n",
        "    #Reset Batch gradient Buffer\n",
        "    g_dict[layer_name]                =  np.zeros_like(weights[layer_name])                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WtL9kCcTCPe_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "0dfeb67f-23a1-4a4a-839e-c23b5cedfa95"
      },
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    #Pre-process the image\n",
        "    env.render()\n",
        "    processed_observations, prev_processed_observation /\n",
        "    = preprocess_observations(\n",
        "        observation,\n",
        "        prev_processed_observations,\n",
        "        input_dimensions\n",
        "    )\n",
        "    \n",
        "    #Send the processed observation through the policy network (neural network)\n",
        "    #To generate the probability of telling the AI to move up\n",
        "    hidden_layer_values, up_probability = apply_neural_nets(processed_observation, weights)\n",
        "    episode_observations.append(processed_observations)\n",
        "    episode_hidden_layer_values-append(hidden_layer_values)\n",
        "    \n",
        "    #Choose an action by flipping an imaginary coin\n",
        "    action = choose_action(up_probability)\n",
        "    \n",
        "    #Carry out the action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    \n",
        "    #Record the results for later learning\n",
        "    reward_sum     += reward\n",
        "    episode_rewards.append(reward)\n",
        "    \n",
        "    #More info:  http://cs231n.github.io/neural-networks-2/#losses\n",
        "    #Gradient per action\n",
        "    fake_label = 1 if action == 2 else 0\n",
        "    loss_function_gradient = fake_label - up_probability\n",
        "    episode_gradient_log_ps.append(loss_function_gradient)\n",
        "    \n",
        "    #Check if the episode is finished\n",
        "    if done:\n",
        "      episode_number += 1\n",
        "      \n",
        "      \"\"\"\n",
        "      Figure out how we learn after the end of and episode. We do this\n",
        "      by computing the policy gradient of the network at the end of each episode. \n",
        "      \n",
        "      Compile all our observations and gradient calculations for the episode. \n",
        "      This allows us to apply our learnings over all the actions in the episode.\n",
        "      \"\"\"\n",
        "      episode_hidden_layer_values = np.vstack(episode_hidden_layer_values)\n",
        "      episode_observations        = np.vstack(episode_observations)\n",
        "      episode_gradient_log_ps     = np.vstack(episode_gradient_log_ps)\n",
        "      episode_rewards             = np.vstack(episode_rewards)\n",
        "      \n",
        "      \"\"\"\n",
        "      Next, we want to learn in such a way that actions taken towards the end of \n",
        "      an episode more heavily influence our learning than actions taken at the beginning. \n",
        "      This is called discounting.\n",
        "      \"\"\"\n",
        "      episode_gradient_log_ps_discounted = discount_with_reward(\n",
        "          episode_gradient_log_ps,\n",
        "          episode_rewards,\n",
        "          gamma\n",
        "      )\n",
        "      gradient = compute_gradient(\n",
        "          episode_gradient_log_ps_discounted,\n",
        "          episode_hidden_layer_values,\n",
        "          episode_observations,\n",
        "          weights\n",
        "      )\n",
        "      \n",
        "      # Sum the gradient for use when we hit the batch size\n",
        "      for layer_name in gradient:\n",
        "          g_dict[layer_name] += gradient[layer_name]\n",
        "      \n",
        "      #Update weights for our Neural Network and implement our learning\n",
        "      if episode_number % batch_size == 0:\n",
        "          update_weights(weights, expectation_g_squared, g_dict, decay_rate, learning_rate)\n",
        "\n",
        "      episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], [] # reset values\n",
        "      observation = env.reset() # reset env\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      print 'resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward)\n",
        "      reward_sum = 0\n",
        "      prev_processed_observations = None\n",
        "      \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-9693fea90ec1>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    processed_observations, prev_processed_observation /\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "uev7ejsbemFy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Save weights to Google drive\n",
        "https://colab.research.google.com/notebooks/io.ipynb\n"
      ]
    }
  ]
}